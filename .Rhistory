group_by(variables) %>%
summarise(
"valores únicos" = n_distinct(valores),
porcentaje_faltantes = sum(is.na(valores))/nrow(data)*100
) %>%
arrange(
desc(porcentaje_faltantes),
"valores únicos"
) # ordenamos por porcentaje de faltantes y valores unicos
tabla_exploratorios
#resumen de los datos
data %>% glimpse()
library(dplyr)
#LECTURA DE LOS DATOS
#data <- read.csv("C:/Users/programadorweb4/Documents/m_d_m/tt1/tt1/train.csv",
#row.names = 'id');
data <- read.csv("C:/Users/programadorweb4/Documents/m_d_m/tt1/tt1/train.csv");
head(data)
testdata <- read.csv("C:/Users/programadorweb4/Documents/m_d_m/tt1/tt1/test.csv");
head(testdata)
#convertimos a factor los datos categóricos
data <- data %>%
mutate(Gender = as.factor(Gender),
Driving_License = as.factor(Driving_License),
Region_Code = as.factor(Region_Code),
Previously_Insured = as.factor(Previously_Insured),
Vehicle_Age = as.factor(Vehicle_Age),
Vehicle_Damage = as.factor(Vehicle_Damage),
Policy_Sales_Channel = as.factor(Policy_Sales_Channel),
Response = as.factor(Response))
#resumen de los datos
data %>% glimpse()
#VALORES ÚNICOS Y FALTANTES
tabla_exploratorios =  data %>%
gather(.,
key = "variables",
value = "valores"
) %>% # agrupamos por las variables del set
group_by(variables) %>%
summarise(
"valores únicos" = n_distinct(valores),
porcentaje_faltantes = sum(is.na(valores))/nrow(data)*100
) %>%
arrange(
desc(porcentaje_faltantes),
"valores únicos"
) # ordenamos por porcentaje de faltantes y valores unicos
tabla_exploratorios
View(tabla_exploratorios)
detach("package:plyr", unload = TRUE)
#VALORES ÚNICOS Y FALTANTES
tabla_exploratorios =  data %>%
gather(.,
key = "variables",
value = "valores"
) %>% # agrupamos por las variables del set
group_by(variables) %>%
summarise(
"valores únicos" = n_distinct(valores),
porcentaje_faltantes = sum(is.na(valores))/nrow(data)*100
) %>%
arrange(
desc(porcentaje_faltantes),
"valores únicos"
) # ordenamos por porcentaje de faltantes y valores unicos
tabla_exploratorios
#cantidad de registros en cada categoría de la variable Policy_Sales_Channel
count(data, "Policy_Sales_Channel") #usa la función de plyr
#PRE PROCESAMIENTO DE LOS DATOS
#one-hot-encoding para xgboost
oh_sdata <- pd.get_dummies(sdata, drop_first = TRUE)
library(caret)
?dummyVars
#PRE PROCESAMIENTO DE LOS DATOS
#one-hot-encoding para xgboost
oh_sdata <- dummyVars("~.", sdata, drop_first = TRUE)
head(oh_sdata)
#PRE PROCESAMIENTO DE LOS DATOS
#one-hot-encoding para xgboost
dummies <- dummyVars("~.", sdata, drop_first = TRUE)
train_dummies <- predict(dummies, newdata = train)
#PRE PROCESAMIENTO DE LOS DATOS
#one-hot-encoding para xgboost
oh_sdata <- sdata
dummies <- dummyVars("~.", oh_sdata, drop_first = TRUE)
train_dummies <- predict(dummies, newdata = oh_sdata)
View(train_dummies)
#PRE PROCESAMIENTO DE LOS DATOS
#one-hot-encoding para xgboost
oh_sdata <- sdata
oh_sdata <- oh_sdata %>% mutate(Response = as.numeric(Response))
dummies <- dummyVars("~ Gender +
Driving_License +
Policy_Sales_Channel +
Previously_Insured +
Region_Code +
Vehicle_Age +
Vehicle_Damage", oh_sdata, drop2nd = TRUE)
train_dummies <- predict(dummies, newdata = oh_sdata)
View(train_dummies)
#PRE PROCESAMIENTO DE LOS DATOS
#one-hot-encoding para xgboost
oh_sdata <- sdata
oh_sdata <- oh_sdata %>% mutate(Response = as.numeric(Response))
dummies <- dummyVars("~.", data = oh_sdata, fullRank = T)
train_dummies <- data.frame(predict(dummies, newdata = oh_sdata))
train_dummies
View(train_dummies)
#FILTROS PARA CONTAR EN CATEGORÍAS
#response
nR1 <- data %>% filter(Response == '1') %>% nrow
nR1
nR0 <- data %>% filter(Response == '0') %>% nrow
nR0
count(data)
count(data)
nR1*100/count(data)
count(data)
count(testdata)
countTrain <- count(data)
countTest <- count(testdata)
percTest <- countTest * 100 / (countTrain + countTest)
percTest
View(testdata)
0.7*countTrain
0.3*countTrain
library(tidyverse)
library(tidymodels)
library(rsample)
library(corrr)
library(knitr)
library(kableExtra)
library(GGally)
library(robustbase)
library(reshape)
library(dplyr)
library(ggcorrplot)
library(plyr)
library(caret)
require("data.table")
require("lightgbm")
#LECTURA DE LOS DATOS
#data <- read.csv("C:/Users/programadorweb4/Documents/m_d_m/tt1/tt1/train.csv",
#row.names = 'id');
#data <- read.csv("C:/Users/programadorweb4/Documents/m_d_m/tt1/tt1/train.csv");
data <- fread("C:/Users/programadorweb4/Documents/m_d_m/tt1/tt1/train.csv",
stringsAsFactors = TRUE);
#columnas con las que se va a entrenar
campos_buenos <- setdiff(colnames(data), c("id", "Response"))
campos_buenos
#split de training y test
set.seed(491)
nrow(data)
sample <- sample(c(TRUE, FALSE), nrow(data), replace=TRUE, prob=c(0.7, 0.3))
dtrain <- data[sample, ]
dtest <- data[!sample, ]
#formato que necesita lightGBM
dtrain_f <- lgb.Dataset(data = data.matrix(dtrain[, campos_buenos, with=FALSE]),
label = dtrain[, Response])
dtest_f <- lgb.Dataset.create.valid(dtrain_f,
data = data.matrix(dtest[, campos_buenos, with=FALSE]),
label = dtest[, Response])
#generar el modelo
#define parameters
params = list(
objective = "binary",
seed = 491
)
#validation data
valids = list(test = dtest_f)
#train model
modelo  <- lgb.train( data= dtrain_f,
param= params,
nrounds = 100,
valids,
early_stopping_rounds = 10
)
print(modelo$best_score)
View(data)
c("Gender", "Driving_License")
c("Gender", "Driving_License", "Region_Code", "Previously_Insured", "Vehicle_Age", "Vehicle_Damage", "Policy_Sales_Channel")
cat_feats <- c("Gender", "Driving_License", "Region_Code", "Previously_Insured",
"Vehicle_Age", "Vehicle_Damage", "Policy_Sales_Channel")
lgb.Dataset.set.categorical(dtrain_f, cat_feats)
dtrain <- data[sample, ]
dtest <- data[!sample, ]
#formato que necesita lightGBM
dtrain_f <- lgb.Dataset(data = data.matrix(dtrain[, campos_buenos, with=FALSE]),
label = dtrain[, Response])
lgb.Dataset.set.categorical(dtrain_f, cat_feats)
dtest_f <- lgb.Dataset.create.valid(dtrain_f,
data = data.matrix(dtest[, campos_buenos, with=FALSE]),
label = dtest[, Response])
lgb.Dataset.set.categorical(dtest_f, cat_feats)
#generar el modelo
#define parameters
params = list(
objective = "binary",
metric = "auc",
is_unbalance = TRUE,
seed = 491
)
#validation data
valids = list(test = dtest_f)
#train model
modelo  <- lgb.train( data= dtrain_f,
param= params,
nrounds = 100,
valids,
early_stopping_rounds = 10,
categorial_feature = cat_feats
)
#train model
modelo  <- lgb.train( data= dtrain_f,
param= params,
nrounds = 100,
valids,
early_stopping_rounds = 10,
categorical_feature = cat_feats
)
print(modelo$best_score)
#prediction
pred = predict(modelo, data.matrix( dtest[, campos_buenos, with=FALSE ]))
pred_y = max.col(pred)-1
pred_y
tb_importancia  <-  as.data.table( lgb.importance(modelo) )
tb_importancia
pred
View(dtest)
dtest[,id]
#prediction
dtest_pred <- data.matrix( dtest[, campos_buenos, with=FALSE ])
pred = predict(modelo, dtest_pred)
preds = data.table(id=dtest[,id], target=predict(modelo, dtest_pred))
colnames(preds)[1] = "id"
preds
print(modelo$best_iter)
#generar el modelo
#define parameters
params = list(
objective = "binary",
metric = "Binary_logloss",
is_unbalance = TRUE,
seed = 491
)
#validation data
valids = list(test = dtest_f)
#train model
modelo  <- lgb.train( data= dtrain_f,
param= params,
nrounds = 100,
valids,
early_stopping_rounds = 10,
categorical_feature = cat_feats
)
print(modelo$best_score)
print(modelo$best_iter)
View(preds)
colnames(pred)
class_names = colnames(pred)[apply(pred, 1, which.max)]
apply(pred, 1, which.max)
dtest[, pred] = preds[, target]
dtest[, pred = preds[, target]]
preds[, target]
dtest[, pred := preds[, target]]
preds[, class := ifelse(target > 0.5, 1, 0)]
preds
confusion_matrix <- table(preds$class, dtest$Response)
confusion_matrix
accuracy <- sum(preds$class == dtest$Response) / nrow(dtest$Response)
accuracy
accuracy
accuracy <- sum(preds$class == dtest$Response) / nrow(dtest$Response)
accuracy
sum(preds$class == dtest$Response)
nrow(dtest$Response)
accuracy <- sum(preds$class == dtest$Response) / nrow(dtest)
accuracy
preds[, class := ifelse(target > 0.7, 1, 0)]
preds
confusion_matrix <- table(preds$class, dtest$Response)
confusion_matrix
accuracy <- sum(preds$class == dtest$Response) / nrow(dtest)
accuracy
preds[, class := ifelse(target > 0.8, 1, 0)]
preds
confusion_matrix <- table(preds$class, dtest$Response)
confusion_matrix
accuracy <- sum(preds$class == dtest$Response) / nrow(dtest)
accuracy
threshold <- lgb.threshold(y_true = dtest$Response, y_score = pred, metric = "accuracy")
roc <- roc(dtest$Response, pred)
library(lightgbm)
threshold <- lgb.threshold(y_true = dtest$Response, y_score = pred, metric = "accuracy")
library(pROC)
roc <- roc(dtest$Response, pred)
threshold <- coords(roc, "best")$thresholds[1]
threshold
threshold
#generar el modelo
#define parameters
params = list(
objective = "binary",
metric = "average_precision",
is_unbalance = TRUE,
seed = 491
)
#validation data
valids = list(test = dtest_f)
#train model
modelo  <- lgb.train( data= dtrain_f,
param= params,
nrounds = 100,
valids,
early_stopping_rounds = 10,
categorical_feature = cat_feats
)
print(modelo$best_score)
#generar el modelo
#define parameters
params = list(
objective = "binary",
metric = "auc",
is_unbalance = TRUE,
seed = 491
)
#validation data
valids = list(test = dtest_f)
#train model
modelo  <- lgb.train( data= dtrain_f,
param= params,
nrounds = 100,
valids,
early_stopping_rounds = 10,
categorical_feature = cat_feats
)
print(modelo$best_score)
#generar el modelo
#define parameters
params = list(
objective = "binary",
metric = "binary_error",
is_unbalance = TRUE,
seed = 491
)
#validation data
valids = list(test = dtest_f)
#train model
modelo  <- lgb.train( data= dtrain_f,
param= params,
nrounds = 100,
valids,
early_stopping_rounds = 10,
categorical_feature = cat_feats
)
print(modelo$best_score)
# Carga de liberías
library(tidyverse)
library(tidymodels)
library(rsample)
library(corrr)
library(knitr)
library(kableExtra)
library(GGally)
library(robustbase)
library(reshape)
library(dplyr)
library(ggcorrplot)
library(plyr)
library(caret)
library(caTools)
library(ROCR)
require("data.table")
require("lightgbm")
rm( list=ls() )  #remove all objects
gc()             #garbage collection
#LECTURA DE LOS DATOS
#data <- read.csv("C:/Users/programadorweb4/Documents/m_d_m/tt1/tt1/train.csv",
#row.names = 'id');
#data <- read.csv("C:/Users/programadorweb4/Documents/m_d_m/tt1/tt1/train.csv");
data <- fread("C:/Users/programadorweb4/Documents/m_d_m/tt1/tt1/train.csv",
stringsAsFactors = TRUE);
#columnas con las que se va a entrenar
campos_buenos <- setdiff(colnames(data), c("id", "Response"))
campos_buenos
#split de training y test
set.seed(491)
nrow(data)
sample <- sample(c(TRUE, FALSE), nrow(data), replace=TRUE, prob=c(0.7, 0.3))
dtrain <- data[sample, ]
dtest <- data[!sample, ]
#formato que necesita lightGBM
cat_feats <- c("Gender", "Driving_License", "Region_Code", "Previously_Insured",
"Vehicle_Age", "Vehicle_Damage", "Policy_Sales_Channel")
dtrain_f <- lgb.Dataset(data = data.matrix(dtrain[, campos_buenos, with=FALSE]),
label = dtrain[, Response])
lgb.Dataset.set.categorical(dtrain_f, cat_feats)
dtest_f <- lgb.Dataset.create.valid(dtrain_f,
data = data.matrix(dtest[, campos_buenos, with=FALSE]),
label = dtest[, Response])
lgb.Dataset.set.categorical(dtest_f, cat_feats)
#generar el modelo
#define parameters
params = list(
objective = "binary",
metric = "auc",
is_unbalance = TRUE,
seed = 491
)
#validation data
valids = list(test = dtest_f)
#train model
modelo  <- lgb.train( data= dtrain_f,
param= params,
nrounds = 100, #alias del num_iterations
valids,
early_stopping_rounds = 10,
categorical_feature = cat_feats
)
print(modelo$best_score)
print(modelo$best_iter)
#prediction
dtest_pred <- data.matrix( dtest[, campos_buenos, with=FALSE ])
pred = predict(modelo, dtest_pred)
roc_obj <- roc(dtest$Response, pred)
require("pROC")
require("MLmetrics")
roc_obj <- roc(dtest$Response, pred)
# Obtener los valores de F1 para diferentes umbrales
coordenadas <- coords(roc_obj, "best", ret=c("threshold", "f1"))
# Obtener el umbral y valor máximo de F1
umbral_max_f1 <- coordenadas$threshold
umbral_max_f1
predicciones <- ifelse(pred >= umbral_max_f1, 1, 0)
# Calcular el valor de F1
f1 <- F1_Score(predicciones, dtest$Response)
f1
confusionMatrix(table(predicciones, dtest$Response))
#umbral de máximo accuracy
ROCR_pred_test <- prediction(pred, dtest$Response)
ROCR_perf_test <- ROCR::performance(ROCR_pred_test,'tpr','fpr')
plot(ROCR_perf_test,colorize=TRUE,print.cutoffs.at=seq(0.1,by=0.1))
cost_perf = ROCR::performance(ROCR_pred_test, "cost")
#para reducir los FN a costo de incrementar los FP, obtiene un mejor accuracy
umbral_max_acc <- ROCR_pred_test@cutoffs[[1]][which.min(cost_perf@y.values[[1]])]
umbral_max_acc
predicciones_2 <- ifelse(pred >= umbral_max_acc, 1, 0)
confusionMatrix(table(predicciones_2, dtest$Response))
tb_importancia  <-  as.data.table( lgb.importance(modelo) )
tb_importancia
params = list(
objective = "binary",
metric = "auc",
is_unbalance = TRUE,
max_depth = -1,
min_gain_to_split = 0.0,
lambda_l1= 0.0,
lambda_l2= 0.0,
learning_rate = 0.0153081329445143,
feature_fraction = 0.48186684122117,
min_data_in_leaf = 23,
num_leaves = 34,
seed = 491
)
#train model
modelo  <- lgb.train( data= dtrain_f,
param= params,
nrounds = 543, #alias del num_iterations
#valids,
early_stopping_rounds = as.integer(50 + 5/0.0153081329445143),
categorical_feature = cat_feats
)
#validation data
valids = list(test = dtest_f)
#train model
modelo  <- lgb.train( data= dtrain_f,
param= params,
nrounds = 543, #alias del num_iterations
valids,
early_stopping_rounds = as.integer(50 + 5/0.0153081329445143),
categorical_feature = cat_feats
)
print(modelo$best_score)
print(modelo$best_iter)
pred = predict(modelo, dtest_pred)
pred2 = predict(modelo, dtest_pred)
roc_obj <- roc(dtest$Response, pred2)
# Obtener los valores de F1 para diferentes umbrales
coordenadas <- coords(roc_obj, "best", ret=c("threshold", "f1"))
# Obtener el umbral y valor máximo de F1
umbral_max_f1_2 <- coordenadas$threshold
umbral_max_f1_2
predicciones_2 <- ifelse(pred >= umbral_max_f1_2, 1, 0)
# Calcular el valor de F1
f1 <- F1_Score(predicciones_2, dtest$Response)
f1
confusionMatrix(table(predicciones_2, dtest$Response))
#umbral de máximo accuracy
ROCR_pred_test <- prediction(pred, dtest$Response)
ROCR_perf_test <- ROCR::performance(ROCR_pred_test,'tpr','fpr')
#umbral de máximo accuracy
ROCR_pred_test <- prediction(pred2, dtest$Response)
ROCR_perf_test <- ROCR::performance(ROCR_pred_test,'tpr','fpr')
plot(ROCR_perf_test, colorize=TRUE, print.cutoffs.at=seq(0.1,by=0.1))
cost_perf = ROCR::performance(ROCR_pred_test, "cost")
#para reducir los FN a costo de incrementar los FP, obtiene un mejor accuracy
umbral_max_acc_2 <- ROCR_pred_test@cutoffs[[1]][which.min(cost_perf@y.values[[1]])]
umbral_max_acc_2
predicciones_3 <- ifelse(pred >= umbral_max_acc_2, 1, 0)
confusionMatrix(table(predicciones_3, dtest$Response))
tb_importancia  <-  as.data.table( lgb.importance(modelo) )
tb_importancia
